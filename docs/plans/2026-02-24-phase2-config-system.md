# OpenKraft Phase 2: Configuration System (.openkraft.yaml)

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Add a `.openkraft.yaml` configuration file that lets users declare their project type, skip irrelevant metrics, customize category weights, exclude paths from scanning, and set minimum thresholds — so OpenKraft produces accurate scores for any Go project, not just API/microservice projects.

**Architecture:** Config as post-processing in the orchestrator. Scorers keep their current pure function signatures unchanged. Config is applied after scoring: skip categories, filter sub-metrics, override weights, recalculate. Zero changes to the 6 scorer files.

**Tech Stack:** Same as Phase 1 (Go 1.24, Cobra, Lipgloss, testify) + `gopkg.in/yaml.v3` (already indirect dep in go.sum)

**Prerequisite:** Phase 1 complete (all 28 tasks done, all tests passing)

---

## Problem Statement

OpenKraft Phase 1 hardcodes scoring rules calibrated for Go API/microservice projects with vertical business modules (orders, payments, users). When run against itself (a CLI tool with global hexagonal layers), it scores **36/100** with multiple false positives:

- **Patterns (20/100):** Penalizes for missing HTTP handlers, repository adapters, and port interfaces — none of which apply to a CLI tool
- **Architecture (15/100):** Expects per-module vertical slices (`internal/{module}/domain/`, `internal/{module}/application/`), doesn't understand global layers (`internal/domain/`, `internal/application/`, `internal/adapters/`)
- **Completeness (36/100):** Picks "adapters" (26 files) as golden module and compares "application" (4 files) against it — they're layers, not comparable business modules

The solution: a `.openkraft.yaml` where users declare `project_type: cli-tool` and get sensible defaults that skip irrelevant metrics and rebalance weights.

---

## Design Decision: Config as Post-Processing

**Why not pass config into each scorer?**

Each of the 6 scorers has a different signature:
```go
ScoreArchitecture(modules, scan, analyzed)
ScoreConventions(scan, analyzed)
ScorePatterns(modules, analyzed)
ScoreTests(scan)
ScoreAIContext(scan)
ScoreCompleteness(modules, analyzed)
```

Adding a `Config` parameter to each would mean modifying 6 function signatures, 6 implementation files, and all their tests. Instead, config is applied **after** scorers return, in the `ScoreService` orchestrator:

1. Scorers run as-is, return their `CategoryScore` with all sub-metrics
2. `applyConfig()` filters skipped categories and sub-metrics
3. Weights are overridden from config
4. Category scores are recalculated proportionally when sub-metrics are removed
5. `ComputeOverallScore()` uses the filtered, re-weighted categories

This keeps scorers pure, testable, and config-free.

---

## Config Schema

```yaml
# .openkraft.yaml — OpenKraft project configuration
# Generated by: openkraft init --type cli-tool

# Project type determines default weights and skipped sub-metrics.
# Options: api (default), cli-tool, library, microservice
project_type: cli-tool

# Category weights (must sum to ~1.0).
# These override the project_type defaults.
# weights:
#   architecture: 0.20
#   conventions: 0.25
#   patterns: 0.10
#   tests: 0.20
#   ai_context: 0.10
#   completeness: 0.15

# Skip entire categories or specific sub-metrics.
# skip:
#   categories:
#     - completeness
#   sub_metrics:
#     - handler_patterns
#     - repository_patterns

# Extra paths to exclude from scanning (in addition to vendor/, .git/, testdata/, etc.)
# exclude_paths:
#   - generated/
#   - third_party/
#   - proto/

# Minimum score thresholds per category (used with --ci flag).
# min_thresholds:
#   tests: 60
#   conventions: 70
#   architecture: 50
```

---

## Project Type Defaults

### `api` (current behavior, default)

No changes from Phase 1. All categories and sub-metrics active with original weights.

| Category | Weight |
|----------|--------|
| architecture | 0.25 |
| conventions | 0.20 |
| patterns | 0.20 |
| tests | 0.15 |
| ai_context | 0.10 |
| completeness | 0.10 |

Skipped sub-metrics: none

### `cli-tool`

CLI tools don't have HTTP handlers, repository adapters, or port interfaces. Emphasis shifts to conventions and tests.

| Category | Weight |
|----------|--------|
| architecture | 0.20 |
| conventions | 0.25 |
| patterns | 0.10 |
| tests | 0.20 |
| ai_context | 0.10 |
| completeness | 0.15 |

Skipped sub-metrics: `handler_patterns`, `repository_patterns`

### `library`

Libraries rarely have handlers, repositories, or ports. Tests and conventions matter most.

| Category | Weight |
|----------|--------|
| architecture | 0.15 |
| conventions | 0.25 |
| patterns | 0.15 |
| tests | 0.25 |
| ai_context | 0.10 |
| completeness | 0.10 |

Skipped sub-metrics: `handler_patterns`, `repository_patterns`, `port_patterns`

### `microservice`

Similar to API but heavier on completeness (all modules should match).

| Category | Weight |
|----------|--------|
| architecture | 0.25 |
| conventions | 0.20 |
| patterns | 0.20 |
| tests | 0.15 |
| ai_context | 0.05 |
| completeness | 0.15 |

Skipped sub-metrics: none

---

## All Available Sub-Metrics (for skip config reference)

### architecture
- `consistent_module_structure` (30 pts)
- `layer_separation` (25 pts)
- `dependency_direction` (20 pts)
- `module_boundary_clarity` (15 pts)
- `architecture_documentation` (10 pts)

### conventions
- `naming_consistency` (30 pts)
- `error_handling` (25 pts)
- `import_ordering` (15 pts)
- `file_organization` (15 pts)
- `code_style` (15 pts)

### patterns
- `entity_patterns` (30 pts)
- `repository_patterns` (25 pts)
- `service_patterns` (20 pts)
- `port_patterns` (15 pts)
- `handler_patterns` (10 pts)

### tests
- `unit_test_presence` (25 pts)
- `integration_tests` (25 pts)
- `test_helpers` (15 pts)
- `test_fixtures` (15 pts)
- `ci_config` (20 pts)

### ai_context
- `claude_md` (25 pts)
- `cursor_rules` (25 pts)
- `agents_md` (25 pts)
- `openkraft_dir` (25 pts)

### completeness
- `file_completeness` (40 pts)
- `structural_completeness` (30 pts)
- `documentation_completeness` (30 pts)

---

## Milestone Map

```
Tasks  1-2:   Foundation (domain config type + port)
Tasks  3-5:   Core (YAML adapter + validation + scanner exclude) [parallelizable]
Tasks  6-7:   Apply Config (score service + check service wiring)
Tasks  8-9:   Init Command (auto-detect + generate config)
Tasks 10-11:  Observability (skipped metric feedback in TUI + config in JSON output)
Task  12:     E2E + Final Verification
```

---

## Task Dependency Graph

```
Task 1 (Domain Config Type)
   │
   └──→ Task 2 (Config Loader Port)
           │
           ├──→ Task 3 (YAML Adapter) ──→ Task 4 (Config Validation) ──┐
           │                                                            │
           └──→ Task 5 (Scanner Exclude Paths) ────────────────────────→├──→ Task 6 (Apply Config in ScoreService)
                                                                        │       │
                                                                        │       └──→ Task 7 (Wire CLI/MCP + CheckService)
                                                                        │               │
                                                                        │               ├──→ Task 10 (Skipped Metric Feedback in TUI)
                                                                        │               │
                                                                        │               └──→ Task 11 (Config in JSON Output)
                                                                        │                       │
           Task 8 (Project Type Auto-Detect) ───────────────────────────┘                       │
                   │                                                                            │
                   └──→ Task 9 (Init Command) ──────────────────────────────────────────────────┤
                                                                                                │
                                                                                                v
                                                                                         Task 12 (E2E + Verify)
```

Parallelizable: Tasks 3+5 can run in parallel after Task 2. Tasks 10+11 can run in parallel after Task 7.

---

## Task 1: Domain Config Type

**Files:**
- Create: `internal/domain/config.go`
- Test: `internal/domain/config_test.go`

**Depends on:** Nothing

The `ProjectConfig` struct lives in domain with zero external dependencies. YAML struct tags are just string annotations — the domain package does NOT import any YAML library.

```go
// internal/domain/config.go
package domain

type ProjectType string

const (
    ProjectTypeAPI          ProjectType = "api"
    ProjectTypeCLI          ProjectType = "cli-tool"
    ProjectTypeLibrary      ProjectType = "library"
    ProjectTypeMicroservice ProjectType = "microservice"
)

type ProjectConfig struct {
    ProjectType   ProjectType        `yaml:"project_type"`
    Weights       map[string]float64 `yaml:"weights,omitempty"`
    Skip          SkipConfig         `yaml:"skip,omitempty"`
    ExcludePaths  []string           `yaml:"exclude_paths,omitempty"`
    MinThresholds map[string]int     `yaml:"min_thresholds,omitempty"`
}

type SkipConfig struct {
    Categories []string `yaml:"categories,omitempty"`
    SubMetrics []string `yaml:"sub_metrics,omitempty"`
}

func DefaultConfig() ProjectConfig { return ProjectConfig{} }

func DefaultConfigForType(pt ProjectType) ProjectConfig { /* preset weights + skips per type */ }

func (c ProjectConfig) IsSkippedCategory(name string) bool { /* check Skip.Categories */ }
func (c ProjectConfig) IsSkippedSubMetric(name string) bool { /* check Skip.SubMetrics */ }
func (c ProjectConfig) EffectiveWeight(category string, defaultWeight float64) float64 { /* Weights[category] or default */ }
```

**Tests:**
```go
func TestDefaultConfig_ChangesNothing(t *testing.T)
func TestDefaultConfigForType_CLI(t *testing.T)          // verify weights and skips
func TestDefaultConfigForType_Library(t *testing.T)
func TestDefaultConfigForType_API(t *testing.T)           // verify no changes from defaults
func TestIsSkippedCategory(t *testing.T)
func TestIsSkippedSubMetric(t *testing.T)
func TestEffectiveWeight_Override(t *testing.T)
func TestEffectiveWeight_FallbackToDefault(t *testing.T)
```

**Step 1:** Write tests

**Step 2:** Implement config.go

**Step 3:** Verify tests pass

```bash
go test ./internal/domain/... -v
```

---

## Task 2: Config Loader Port

**Files:**
- Modify: `internal/domain/ports.go`

**Depends on:** Task 1

Add the `ConfigLoader` interface to ports.go:

```go
// ConfigLoader loads project configuration from the project directory.
type ConfigLoader interface {
    Load(projectPath string) (ProjectConfig, error)
}
```

No test needed — it's just an interface declaration.

```bash
go build ./internal/domain/...
```

---

## Task 3: YAML Config Adapter

**Files:**
- Create: `internal/adapters/outbound/config/yaml_loader.go`
- Test: `internal/adapters/outbound/config/yaml_loader_test.go`

**Depends on:** Tasks 1, 2

Implements `domain.ConfigLoader`. Reads `.openkraft.yaml` from project root. Missing file returns `DefaultConfig()` with no error (backward compatible).

If `project_type` is set in the YAML, load type defaults first, then merge explicit values on top:

```go
func (l *YAMLLoader) Load(projectPath string) (domain.ProjectConfig, error) {
    data, err := os.ReadFile(filepath.Join(projectPath, ".openkraft.yaml"))
    if errors.Is(err, os.ErrNotExist) {
        return domain.DefaultConfig(), nil  // no config = current behavior
    }
    // parse YAML
    // if project_type set, merge: type defaults ← explicit YAML values
}
```

**Tests (use t.TempDir()):**
```go
func TestYAMLLoader_MissingFileReturnsDefaults(t *testing.T)
func TestYAMLLoader_ValidYAML(t *testing.T)
func TestYAMLLoader_InvalidYAML(t *testing.T)
func TestYAMLLoader_ProjectTypeMergesDefaults(t *testing.T)
func TestYAMLLoader_ExplicitWeightsOverrideTypeDefaults(t *testing.T)
func TestYAMLLoader_ExplicitSkipsOverrideTypeDefaults(t *testing.T)
func TestYAMLLoader_ExcludePaths(t *testing.T)
func TestYAMLLoader_MinThresholds(t *testing.T)
```

**Step 1:** Write tests

**Step 2:** Implement yaml_loader.go

**Step 3:** Verify

```bash
go test ./internal/adapters/outbound/config/... -v
```

---

## Task 4: Config Validation

**Files:**
- Modify: `internal/domain/config.go` — add `Validate() error` method
- Modify: `internal/domain/config_test.go` — add validation tests
- Modify: `internal/adapters/outbound/config/yaml_loader.go` — call Validate after parsing

**Depends on:** Tasks 1, 3

The config loaded from YAML must be validated before use. Validation rules:

```go
func (c ProjectConfig) Validate() error {
    // 1. project_type must be a known value (or empty for defaults)
    // 2. weights values must be between 0.0 and 1.0
    // 3. weights must sum to ~1.0 (tolerance ±0.05) if provided
    // 4. weights keys must be valid category names
    // 5. skip.categories entries must be valid category names
    // 6. skip.sub_metrics entries must be valid sub-metric names
    // 7. min_thresholds values must be between 0 and 100
    // 8. min_thresholds keys must be valid category names
    // 9. cannot skip ALL categories (must have at least one active)
}
```

Valid category names: `architecture`, `conventions`, `patterns`, `tests`, `ai_context`, `completeness`

Valid sub-metric names: all 28 sub-metrics listed in the "All Available Sub-Metrics" section above.

The YAML loader calls `cfg.Validate()` after parsing and returns the validation error to the user with a clear message like:

```
error: invalid .openkraft.yaml: unknown project_type "webapp" (valid: api, cli-tool, library, microservice)
error: invalid .openkraft.yaml: weights sum to 1.30 (must be between 0.95 and 1.05)
error: invalid .openkraft.yaml: unknown sub-metric "http_handlers" in skip.sub_metrics (did you mean "handler_patterns"?)
```

**Tests:**
```go
func TestValidate_ValidConfig(t *testing.T)
func TestValidate_UnknownProjectType(t *testing.T)
func TestValidate_WeightsSumTooHigh(t *testing.T)
func TestValidate_WeightsSumTooLow(t *testing.T)
func TestValidate_InvalidCategoryName(t *testing.T)
func TestValidate_InvalidSubMetricName(t *testing.T)
func TestValidate_ThresholdOutOfRange(t *testing.T)
func TestValidate_AllCategoriesSkipped(t *testing.T)
func TestValidate_EmptyConfigIsValid(t *testing.T)
func TestValidate_PartialWeightsValid(t *testing.T)       // only some weights specified = OK
```

**Step 1:** Write validation tests

**Step 2:** Implement Validate() with clear error messages

**Step 3:** Wire into YAML loader (call Validate after Unmarshal)

**Step 4:** Verify

```bash
go test ./internal/domain/... ./internal/adapters/outbound/config/... -v
```

---

## Task 5: Scanner Exclude Paths

**Files:**
- Modify: `internal/domain/ports.go` — change `Scan` signature
- Modify: `internal/adapters/outbound/scanner/scanner.go`
- Modify: `internal/adapters/outbound/scanner/scanner_test.go`

**Depends on:** Task 2

Change the `ProjectScanner` interface to accept optional exclude paths:

```go
type ProjectScanner interface {
    Scan(projectPath string, excludePaths ...string) (*ScanResult, error)
}
```

The variadic parameter means **all existing callers compile without changes**.

**Scanner implementation:**
```go
func (s *FileScanner) Scan(projectPath string, excludePaths ...string) (*domain.ScanResult, error) {
    // Build combined skip set
    extraSkip := make(map[string]bool)
    for _, p := range excludePaths {
        extraSkip[p] = true
    }

    err = filepath.WalkDir(absPath, func(path string, d os.DirEntry, err error) error {
        if d.IsDir() {
            if skipDirs[d.Name()] || extraSkip[d.Name()] {
                return filepath.SkipDir
            }
        }
        // ... rest unchanged ...
    })
}
```

**Tests:**
```go
func TestFileScanner_CustomExcludePaths(t *testing.T)  // create temp dir with excluded subdir, verify it's skipped
func TestFileScanner_NoExcludePathsBackwardCompatible(t *testing.T)  // existing behavior unchanged
```

**Step 1:** Write tests

**Step 2:** Update interface + implementation

**Step 3:** Verify

```bash
go test ./internal/adapters/outbound/scanner/... -v
```

---

## Task 6: Apply Config in ScoreService

**Files:**
- Modify: `internal/application/score_service.go`
- Modify: `internal/application/score_service_test.go`

**Depends on:** Tasks 3, 4, 5

This is the core change. Add `configLoader` to `ScoreService` and apply config after scoring.

**ScoreService changes:**
```go
type ScoreService struct {
    scanner      domain.ProjectScanner
    detector     domain.ModuleDetector
    analyzer     domain.CodeAnalyzer
    configLoader domain.ConfigLoader  // NEW
}

func NewScoreService(
    scanner domain.ProjectScanner,
    detector domain.ModuleDetector,
    analyzer domain.CodeAnalyzer,
    configLoader domain.ConfigLoader,  // NEW
) *ScoreService

func (s *ScoreService) ScoreProject(projectPath string) (*domain.Score, error) {
    cfg, err := s.configLoader.Load(projectPath)
    // ... existing scan, detect, analyze, score ...
    // pass cfg.ExcludePaths to scanner
    scan, err := s.scanner.Scan(projectPath, cfg.ExcludePaths...)
    // ... run 6 scorers ...
    categories = applyConfig(categories, cfg)
    overall := domain.ComputeOverallScore(categories)
    // ...
}
```

**applyConfig function:**
```go
func applyConfig(categories []domain.CategoryScore, cfg domain.ProjectConfig) []domain.CategoryScore {
    var result []domain.CategoryScore
    for _, cat := range categories {
        // 1. Skip entire category if configured
        if cfg.IsSkippedCategory(cat.Name) {
            continue
        }

        // 2. Override weight
        cat.Weight = cfg.EffectiveWeight(cat.Name, cat.Weight)

        // 3. Filter skipped sub-metrics, mark them as skipped
        var kept []domain.SubMetric
        for _, sm := range cat.SubMetrics {
            if cfg.IsSkippedSubMetric(sm.Name) {
                // Mark as skipped (for TUI feedback in Task 10)
                sm.Skipped = true
                sm.Detail = "skipped by config"
            }
            kept = append(kept, sm)
        }

        // 4. Recalculate category score excluding skipped sub-metrics
        cat.SubMetrics = kept
        activePoints, activeScore := 0, 0
        for _, sm := range kept {
            if !sm.Skipped {
                activePoints += sm.Points
                activeScore += sm.Score
            }
        }
        if activePoints > 0 {
            cat.Score = activeScore * 100 / activePoints
        }

        // 5. Check min threshold
        if min, ok := cfg.MinThresholds[cat.Name]; ok && cat.Score < min {
            cat.Issues = append(cat.Issues, domain.Issue{
                Severity: domain.SeverityError,
                Category: cat.Name,
                Message:  fmt.Sprintf("score %d is below configured minimum %d", cat.Score, min),
            })
        }

        result = append(result, cat)
    }
    return result
}
```

**Note:** This requires adding a `Skipped bool` field to `domain.SubMetric` (see Task 10 for TUI rendering). Add it here so `applyConfig` can mark skipped metrics.

**Tests:**
```go
func TestScoreService_DefaultConfigSameAsPhase1(t *testing.T)     // no config file = identical scores
func TestScoreService_CLITypeSkipsHandlerPatterns(t *testing.T)    // patterns score should improve
func TestScoreService_CustomWeightsApplied(t *testing.T)           // verify weight override
func TestScoreService_SkippedCategoryExcluded(t *testing.T)        // category removed from results
func TestScoreService_MinThresholdGeneratesIssue(t *testing.T)     // issue added when below min
func TestScoreService_SubMetricRemovalRecalculates(t *testing.T)   // score scales correctly
func TestScoreService_ExcludePathsPassedToScanner(t *testing.T)    // verify scanner receives excludes
```

**Important:** Existing tests that create `NewScoreService(scanner, detector, parser)` must be updated to `NewScoreService(scanner, detector, parser, configLoader)`. Use a simple stub config loader that returns `DefaultConfig()`.

**Step 1:** Add `Skipped bool` to `domain.SubMetric`

**Step 2:** Write tests (including updating existing test signatures)

**Step 3:** Implement applyConfig and ScoreService changes

**Step 4:** Verify all tests pass

```bash
go test ./internal/application/... -v
```

---

## Task 7: Wire Config into CLI, MCP, and CheckService

**Files:**
- Modify: `internal/adapters/inbound/cli/score.go`
- Modify: `internal/adapters/inbound/cli/check.go`
- Modify: `internal/adapters/inbound/mcp/tools.go`
- Modify: `internal/application/check_service.go` (add configLoader)
- Modify: `internal/application/check_service_test.go` (update constructor)

**Depends on:** Task 6

Update all places that construct `ScoreService` and `CheckService` to pass `config.New()`:

**score.go:**
```go
import "github.com/openkraft/openkraft/internal/adapters/outbound/config"

svc := application.NewScoreService(
    scanner.New(),
    detector.New(),
    parser.New(),
    config.New(),  // NEW
)
```

**check.go:** Same pattern. CheckService also gets `configLoader` for passing `ExcludePaths` to scanner.

**mcp/tools.go:** Same pattern for both services used in MCP tool handlers.

**check_service.go changes:**
```go
type CheckService struct {
    scanner      domain.ProjectScanner
    detector     domain.ModuleDetector
    analyzer     domain.CodeAnalyzer
    configLoader domain.ConfigLoader  // NEW
}
// Load config, pass cfg.ExcludePaths... to scanner.Scan()
```

**Step 1:** Update all call sites and constructors

**Step 2:** Verify all tests pass

```bash
make test
```

---

## Task 8: Project Type Auto-Detection

**Files:**
- Create: `internal/domain/detect_type.go`
- Test: `internal/domain/detect_type_test.go`

**Depends on:** Task 1

Instead of forcing users to know if their project is a `cli-tool` or `api`, OpenKraft auto-detects the project type by analyzing the scan result and analyzed files. This is a pure domain function (no deps).

```go
// internal/domain/detect_type.go
package domain

// DetectProjectType infers the project type from scan results and analyzed files.
// Returns the best-guess ProjectType and a confidence string ("high", "medium", "low").
func DetectProjectType(scan *ScanResult, modules []DetectedModule, analyzed map[string]*AnalyzedFile) (ProjectType, string) {
    // Heuristics (checked in order):

    // 1. CLI tool: has cmd/ or main.go with cobra/urfave/kingpin imports
    //    → ProjectTypeCLI, "high"

    // 2. Library: no cmd/, no main.go, has go.mod
    //    → ProjectTypeLibrary, "high"

    // 3. Microservice: has internal/{module}/ with 4+ modules, each with domain/application layers
    //    → ProjectTypeMicroservice, "medium"

    // 4. API: has HTTP handler files (handler.go, routes.go), net/http or gin/echo/fiber imports
    //    → ProjectTypeAPI, "high"

    // 5. Default fallback
    //    → ProjectTypeAPI, "low"
}
```

**Detection signals (per heuristic):**

| Signal | Detected via | Indicates |
|--------|-------------|-----------|
| `cmd/` directory exists | `scan.AllFiles` has `cmd/` prefix | CLI or API with binary |
| cobra/urfave import | `analyzed[*].Imports` contains cobra | CLI tool |
| No `cmd/` or `main.go` | Absence in `scan.AllFiles` | Library |
| `net/http`, gin, echo import | `analyzed[*].Imports` | API |
| 4+ modules with layers | `len(modules) >= 4` with mixed layers | Microservice |
| handler.go, routes.go files | `scan.GoFiles` suffix matching | API |

**Tests:**
```go
func TestDetectProjectType_CLIWithCobra(t *testing.T)        // cmd/ + cobra import → cli-tool
func TestDetectProjectType_LibraryNoCmd(t *testing.T)         // no cmd/, no main → library
func TestDetectProjectType_APIWithHandlers(t *testing.T)      // handler files + http imports → api
func TestDetectProjectType_MicroserviceMultiModule(t *testing.T)  // 4+ modules → microservice
func TestDetectProjectType_FallbackToAPI(t *testing.T)        // ambiguous → api with low confidence
```

**Step 1:** Write tests with stubbed ScanResult/modules/analyzed

**Step 2:** Implement DetectProjectType

**Step 3:** Verify

```bash
go test ./internal/domain/... -v
```

---

## Task 9: `openkraft init` Command with Auto-Detect

**Files:**
- Create: `internal/adapters/inbound/cli/init.go`
- Test: `internal/adapters/inbound/cli/init_test.go`
- Modify: `internal/adapters/inbound/cli/root.go` — add `cmd.AddCommand(newInitCmd())`

**Depends on:** Tasks 1, 8

Generates a `.openkraft.yaml` with commented defaults. When `--type` is not specified, runs auto-detection and suggests the project type:

```
$ openkraft init
Detected project type: cli-tool (high confidence)
Generated .openkraft.yaml with cli-tool defaults.

$ openkraft init --type api
Generated .openkraft.yaml with api defaults.
```

```go
func newInitCmd() *cobra.Command {
    var (
        projectType string
        force       bool
    )

    cmd := &cobra.Command{
        Use:   "init [path]",
        Short: "Generate a default .openkraft.yaml configuration",
        Long:  "Create an .openkraft.yaml file with sensible defaults. Auto-detects project type if --type is not specified.",
        Args:  cobra.MaximumNArgs(1),
        RunE: func(cmd *cobra.Command, args []string) error {
            path := "."
            if len(args) > 0 { path = args[0] }

            configPath := filepath.Join(path, ".openkraft.yaml")

            // Check if file exists (unless --force)
            if !force {
                if _, err := os.Stat(configPath); err == nil {
                    return fmt.Errorf(".openkraft.yaml already exists (use --force to overwrite)")
                }
            }

            // Auto-detect if --type not specified
            pt := domain.ProjectType(projectType)
            if projectType == "" {
                // Run scanner + detector + analyzer to detect type
                // Print: "Detected project type: X (confidence)"
            }

            // Generate config content with comments
            content := generateConfigYAML(pt)
            return os.WriteFile(configPath, []byte(content), 0644)
        },
    }

    cmd.Flags().StringVar(&projectType, "type", "", "Project type: api, cli-tool, library, microservice (auto-detected if omitted)")
    cmd.Flags().BoolVar(&force, "force", false, "Overwrite existing .openkraft.yaml")

    return cmd
}
```

The generated YAML should include comments explaining each field:

```yaml
# OpenKraft Configuration
# Docs: https://github.com/openkraft/openkraft
# Detected project type: cli-tool (high confidence)

project_type: cli-tool

# Category weights (must sum to ~1.0)
weights:
  architecture: 0.20
  conventions: 0.25
  patterns: 0.10
  tests: 0.20
  ai_context: 0.10
  completeness: 0.15

# Skip sub-metrics irrelevant to this project type
skip:
  sub_metrics:
    - handler_patterns
    - repository_patterns
  # categories:
  #   - completeness

# Extra paths to exclude from scanning
# exclude_paths:
#   - generated/
#   - third_party/

# Minimum score thresholds (used with --ci flag)
# min_thresholds:
#   tests: 60
#   conventions: 70
```

**Tests:**
```go
func TestInitCommand_GeneratesConfig(t *testing.T)          // creates .openkraft.yaml
func TestInitCommand_FailsIfExists(t *testing.T)            // error without --force
func TestInitCommand_ForceOverwrites(t *testing.T)           // works with --force
func TestInitCommand_ExplicitType(t *testing.T)              // --type cli-tool content
func TestInitCommand_GeneratedYAMLIsValid(t *testing.T)      // parse back and verify structure
func TestInitCommand_AutoDetect(t *testing.T)                // no --type, runs detection
```

**Step 1:** Write tests

**Step 2:** Implement init.go + generateConfigYAML + register in root.go

**Step 3:** Verify

```bash
go test ./internal/adapters/inbound/cli/... -v
```

---

## Task 10: Skipped Metric Feedback in TUI

**Files:**
- Modify: `internal/domain/model.go` — add `Skipped bool` to `SubMetric`
- Modify: `internal/adapters/outbound/tui/renderer.go` — render skipped metrics visually
- Modify: `internal/adapters/outbound/tui/renderer_test.go` (if exists)

**Depends on:** Task 6

When a sub-metric is skipped by config, the TUI should show it clearly instead of silently removing it. Users need to understand why their score changed.

**SubMetric change:**
```go
type SubMetric struct {
    Name    string `json:"name"`
    Score   int    `json:"score"`
    Points  int    `json:"points"`
    Detail  string `json:"detail,omitempty"`
    Skipped bool   `json:"skipped,omitempty"`  // NEW
}
```

**TUI rendering for skipped metrics:**
```
  architecture           ██████████░░░░░  70/100  (weight: 20%)
  conventions            █████████████░░  89/100  (weight: 25%)
  patterns               ████████████░░░  82/100  (weight: 10%)
    ⊘ handler_patterns    — skipped by config
    ⊘ repository_patterns — skipped by config
  tests                  ████████░░░░░░░  55/100  (weight: 20%)
```

Skipped metrics are shown in dim/gray with `⊘` prefix, indented under their category. They don't affect the category score but the user can see they exist and are intentionally excluded.

**Implementation in renderer.go:**
```go
func renderCategory(cat domain.CategoryScore) string {
    // ... existing bar rendering ...

    // After the bar, show skipped sub-metrics
    for _, sm := range cat.SubMetrics {
        if sm.Skipped {
            // render dim: "    ⊘ {name} — skipped by config"
        }
    }
}
```

**Tests:**
```go
func TestRenderScore_ShowsSkippedMetrics(t *testing.T)
func TestRenderScore_NoSkippedMetrics_UnchangedOutput(t *testing.T)
```

**Step 1:** Add `Skipped` field to SubMetric

**Step 2:** Update renderer

**Step 3:** Verify

```bash
go test ./internal/adapters/outbound/tui/... ./internal/domain/... -v
```

---

## Task 11: Config in JSON Output

**Files:**
- Modify: `internal/domain/model.go` — add `AppliedConfig` field to `Score`
- Modify: `internal/application/score_service.go` — attach config to score result

**Depends on:** Task 6

When running `openkraft score --json`, the output should include what config was applied so two different runs can be compared and understood.

**Score struct change:**
```go
type Score struct {
    Overall    int             `json:"overall"`
    Categories []CategoryScore `json:"categories"`
    Timestamp  string          `json:"timestamp,omitempty"`
    CommitHash string          `json:"commit_hash,omitempty"`
    Config     *AppliedConfig  `json:"config,omitempty"`     // NEW
}

type AppliedConfig struct {
    ProjectType      string             `json:"project_type,omitempty"`
    Weights          map[string]float64 `json:"weights,omitempty"`
    SkippedCategories []string          `json:"skipped_categories,omitempty"`
    SkippedSubMetrics []string          `json:"skipped_sub_metrics,omitempty"`
    ExcludePaths     []string           `json:"exclude_paths,omitempty"`
    Source           string             `json:"source"`  // "file", "auto-detected", "defaults"
}
```

**Example JSON output:**
```json
{
  "overall": 68,
  "categories": [ ... ],
  "config": {
    "project_type": "cli-tool",
    "weights": {
      "architecture": 0.20,
      "conventions": 0.25,
      "patterns": 0.10,
      "tests": 0.20,
      "ai_context": 0.10,
      "completeness": 0.15
    },
    "skipped_sub_metrics": ["handler_patterns", "repository_patterns"],
    "source": "file"
  }
}
```

The `source` field tells the user where the config came from:
- `"file"` — loaded from `.openkraft.yaml`
- `"defaults"` — no config file, using hardcoded defaults

**ScoreService changes:**
```go
// After applyConfig, attach config info to score
score.Config = &domain.AppliedConfig{
    ProjectType:       string(cfg.ProjectType),
    Weights:           effectiveWeights,
    SkippedCategories: cfg.Skip.Categories,
    SkippedSubMetrics: cfg.Skip.SubMetrics,
    ExcludePaths:      cfg.ExcludePaths,
    Source:            configSource,  // "file" or "defaults"
}
```

**Tests:**
```go
func TestScoreJSON_IncludesConfig(t *testing.T)             // config present in JSON
func TestScoreJSON_DefaultsShowSource(t *testing.T)          // source = "defaults" when no file
func TestScoreJSON_FileShowsSource(t *testing.T)             // source = "file" when .openkraft.yaml exists
func TestScoreJSON_SkippedMetricsListed(t *testing.T)        // skipped sub-metrics in config section
```

**Step 1:** Add `AppliedConfig` type and `Config` field to Score

**Step 2:** Attach config in ScoreService after applying

**Step 3:** Verify JSON output includes config

```bash
go test ./internal/application/... ./internal/domain/... -v
```

---

## Task 12: E2E Config Tests + Final Verification

**Files:**
- Modify: `tests/e2e/e2e_test.go`

**Depends on:** All previous tasks

**E2E tests:**
```go
// Init command
func TestE2E_Init(t *testing.T)                    // openkraft init --type cli-tool generates valid file
func TestE2E_InitAutoDetect(t *testing.T)           // openkraft init without --type auto-detects
func TestE2E_InitForce(t *testing.T)               // openkraft init --force overwrites existing
func TestE2E_InitFailsIfExists(t *testing.T)       // openkraft init fails if file exists

// Config validation
func TestE2E_InvalidConfigShowsError(t *testing.T) // bad YAML shows clear validation error

// Score with config
func TestE2E_ScoreWithConfig(t *testing.T)          // score with .openkraft.yaml changes weights
func TestE2E_ScoreWithoutConfig(t *testing.T)       // score without config = backward compatible
func TestE2E_ScoreCLIType(t *testing.T)             // cli-tool type skips handler/repository patterns

// JSON output
func TestE2E_ScoreJSONIncludesConfig(t *testing.T)  // --json output has config section
func TestE2E_ScoreJSONDefaultsSource(t *testing.T)  // source = "defaults" without config file

// TUI feedback
func TestE2E_ScoreShowsSkippedMetrics(t *testing.T) // TUI output shows skipped metrics
```

**Final verification checklist:**

```bash
# 1. All tests pass
make test

# 2. Build succeeds
make build

# 3. Init auto-detects project type
./bin/openkraft init
# Expected: "Detected project type: cli-tool (high confidence)"

# 4. Score without config (backward compatible)
rm -f .openkraft.yaml
./bin/openkraft score testdata/go-hexagonal/perfect --json
# Expected: same scores as Phase 1, config.source = "defaults"

# 5. Score with cli-tool config
./bin/openkraft init --type cli-tool --force
./bin/openkraft score .
# Expected: skipped handler_patterns and repository_patterns shown in TUI
# Expected: score higher than 36/100

# 6. Score JSON includes config
./bin/openkraft score . --json
# Expected: "config" section with project_type, weights, skipped_sub_metrics

# 7. Invalid config shows clear error
echo "project_type: invalid_type" > .openkraft.yaml
./bin/openkraft score .
# Expected: "error: invalid .openkraft.yaml: unknown project_type..."

# 8. Cleanup and commit
rm .openkraft.yaml
```

---

## Verification Matrix

| Scenario | Expected |
|----------|----------|
| No `.openkraft.yaml` exists | Identical to Phase 1 behavior |
| `project_type: api` | Identical to Phase 1 behavior |
| `project_type: cli-tool` | handler_patterns and repository_patterns skipped (shown in TUI), weights rebalanced |
| `project_type: library` | handler/repository/port patterns skipped, tests weight increased |
| `project_type: invalid` | Clear validation error with valid options listed |
| Custom `weights` that sum to 1.5 | Validation error: "weights sum to 1.50 (must be between 0.95 and 1.05)" |
| Unknown sub-metric in skip | Validation error: "unknown sub-metric X (did you mean Y?)" |
| Custom `weights` override type defaults | Custom weights used |
| Custom `skip.sub_metrics` override type defaults | Custom skips used |
| `skip.categories: [completeness]` | Completeness category not in output |
| `exclude_paths: [generated/]` | generated/ dir not scanned |
| `min_thresholds.tests: 60` with tests at 50 | Issue generated with severity error |
| `openkraft init` (no --type) | Auto-detects project type, shows confidence |
| `openkraft init --type cli-tool` | `.openkraft.yaml` created with CLI defaults |
| `openkraft init` when file exists | Error: "already exists (use --force)" |
| `openkraft init --force` when file exists | File overwritten |
| `openkraft score --json` with config | JSON includes `config` section with source, weights, skipped metrics |
| `openkraft score --json` without config | JSON includes `config.source: "defaults"` |
| TUI output with skipped metrics | Shows `⊘ handler_patterns — skipped by config` under category |
| TUI output without config | Unchanged from Phase 1 |

---

## Files Summary

### New Files (8)
- `internal/domain/config.go` — ProjectConfig type, defaults, validation, helpers
- `internal/domain/config_test.go`
- `internal/domain/detect_type.go` — Project type auto-detection
- `internal/domain/detect_type_test.go`
- `internal/adapters/outbound/config/yaml_loader.go` — YAML config adapter
- `internal/adapters/outbound/config/yaml_loader_test.go`
- `internal/adapters/inbound/cli/init.go` — Init command with auto-detect
- `internal/adapters/inbound/cli/init_test.go`

### Modified Files (11)
- `internal/domain/ports.go` — add ConfigLoader interface, update Scanner signature
- `internal/domain/model.go` — add Skipped to SubMetric, AppliedConfig to Score
- `internal/application/score_service.go` — add configLoader, applyConfig(), attach config to output
- `internal/application/score_service_test.go` — update constructor calls, add config tests
- `internal/application/check_service.go` — add configLoader, pass excludePaths
- `internal/application/check_service_test.go` — update constructor calls
- `internal/adapters/outbound/scanner/scanner.go` — accept excludePaths variadic
- `internal/adapters/outbound/scanner/scanner_test.go` — add exclude paths test
- `internal/adapters/outbound/tui/renderer.go` — render skipped metrics
- `internal/adapters/inbound/cli/root.go` — add initCmd
- `internal/adapters/inbound/cli/score.go` — wire config.New()
- `internal/adapters/inbound/cli/check.go` — wire config.New()
- `internal/adapters/inbound/mcp/tools.go` — wire config.New()
- `tests/e2e/e2e_test.go` — add config + init E2E tests
